{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Dvm6MmIGddQw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgcJ2LJ2ddQz"
   },
   "source": [
    "## 1. Q-learning in the wild (3 pts)\n",
    "\n",
    "Here we use the qlearning agent on taxi env from openai gym.\n",
    "You will need to insert a few agent functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "c6uJVRBJddQ1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      8\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m     12\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTaxi-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(f\"Taxi-v3 created\")\n",
    "print(f\"Actions: {n_actions}\")\n",
    "\n",
    "class QLearningAgent():\n",
    "    def __init__(self, alpha, epsilon, discount, getLegalActions):\n",
    "        self.getLegalActions = getLegalActions\n",
    "        self._qValues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        if not (state in self._qValues) or not (action in self._qValues[state]):\n",
    "            return 0.0\n",
    "        return self._qValues[state][action]\n",
    "\n",
    "    def setQValue(self, state, action, value):\n",
    "        self._qValues[state][action] = value\n",
    "\n",
    "    def getValue(self, state):\n",
    "        possibleActions = self.getLegalActions(state)\n",
    "        if not possibleActions:\n",
    "            return 0.0\n",
    "        return max(self.getQValue(state, a) for a in possibleActions)\n",
    "\n",
    "    def getPolicy(self, state):\n",
    "        possibleActions = self.getLegalActions(state)\n",
    "        if not possibleActions:\n",
    "            return None\n",
    "        \n",
    "        best_value = -float('inf')\n",
    "        best_action = None\n",
    "        for action in possibleActions:\n",
    "            value = self.getQValue(state, action)\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def getAction(self, state):\n",
    "        possibleActions = self.getLegalActions(state)\n",
    "        if not possibleActions:\n",
    "            return None\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(possibleActions)\n",
    "        else:\n",
    "            return self.getPolicy(state)\n",
    "\n",
    "    def update(self, state, action, nextState, reward):\n",
    "        old_q = self.getQValue(state, action)\n",
    "        next_q = self.getValue(nextState)\n",
    "        new_q = old_q + self.alpha * (reward + self.discount * next_q - old_q)\n",
    "        self.setQValue(state, action, new_q)\n",
    "\n",
    "def play_and_train(env, agent, t_max=1000):\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.getAction(s)\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        agent.update(s, a, next_s, r)\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    alpha=0.2,\n",
    "    epsilon=0.3,\n",
    "    discount=0.95,\n",
    "    getLegalActions=lambda s: range(n_actions)\n",
    ")\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    reward = play_and_train(env, agent)\n",
    "    rewards.append(reward)\n",
    "    agent.epsilon = max(0.01, agent.epsilon * 0.999)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        avg_reward = np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards)\n",
    "        print(f\"Episode {i}\")\n",
    "        print(f\"Reward: {reward:.2f}\")\n",
    "        print(f\"Avg (last 100): {avg_reward:.2f}\")\n",
    "        print(f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Training Progress')\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "print(f\"Final avg: {np.mean(rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IyXE8SzddQ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Устанавливаю gymnasium через conda...\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TFzQrJJddQ2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m---> 29\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(play_and_train(env, agent))\n\u001b[0;32m     30\u001b[0m     agent\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m  \u001b[38;5;66;03m# уменьшайте epsilon со временем\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    \"\"\"This function should\n",
    "    - run a full game, actions given by agent.getAction(s)\n",
    "    - train agent using agent.update(...) whenever possible\n",
    "    - return total reward\"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = agent.getAction(s)\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "\n",
    "        agent.update(s, a, next_s, r)\n",
    "\n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    agent.epsilon *= 0.999 \n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print(f\"Epsilon: {agent.epsilon:.4f}\")\n",
    "        print(f\"Average reward: {np.mean(rewards[-100:]):.2f}\")\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Training Progress')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "gxQu70I_ddQ4"
   },
   "source": [
    "## 3. Continuous state space (2 pt)\n",
    "\n",
    "Чтобы использовать табличный q-learning на continuous состояниях, надо как-то их обрабатывать и бинаризовать. Придумайте способ разбивки на дискретные состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4eD9gTjddQ4"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "n_actions = env.action_space.n\n",
    "print(\"first state:%s\" % (env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6V68sMQddQ4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUM1t0jvddQ4"
   },
   "source": [
    "### Play a few games\n",
    "\n",
    "Постройте распределения различных частей состояния игры. Сыграйте несколько игр и запишите все состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnHeBx5XddQ5"
   },
   "outputs": [],
   "source": [
    "states = []\n",
    "for _ in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        states.append(next_state)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "states = np.array(states)\n",
    "print(f\"Collected {len(states)} states\")\n",
    "print(f\"State ranges: min={states.min(axis=0)}, max={states.max(axis=0)}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "for i in range(4):\n",
    "    ax = axes[i//2, i%2]\n",
    "    ax.hist(states[:, i], bins=30)\n",
    "    ax.set_title(f'Dimension {i+1}')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zZHwCUfddQ5"
   },
   "source": [
    "## Binarize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6f-DyizddQ5"
   },
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "\n",
    "class Binarizer(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.bins = [\n",
    "            np.linspace(-2.4, 2.4, 8),    # позиция\n",
    "            np.linspace(-3.0, 3.0, 6),    # скорость\n",
    "            np.linspace(-0.2, 0.2, 8),    # угол\n",
    "            np.linspace(-3.0, 3.0, 6)     # угл скорость\n",
    "        ]\n",
    "\n",
    "    def to_bin(self, value, bins):\n",
    "        return np.digitize(value, bins)\n",
    "\n",
    "    def observation(self, state):\n",
    "        binned_state = tuple(\n",
    "            self.to_bin(state[i], self.bins[i]) \n",
    "            for i in range(4)\n",
    "        )\n",
    "        return binned_state\n",
    "\n",
    "env = Binarizer(gym.make(\"CartPole-v1\"))\n",
    "print(\"Binarized state example:\", env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkZCEBD_ddQ5"
   },
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sFMtxZYddQ5"
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(\n",
    "    alpha=0.1,\n",
    "    epsilon=0.5,\n",
    "    discount=0.95,\n",
    "    getLegalActions=lambda s: range(n_actions)\n",
    ")\n",
    "\n",
    "rewards = []\n",
    "rewBuf = []\n",
    "for i in range(500):\n",
    "    for _ in range(10):\n",
    "        reward = play_and_train(env, agent)\n",
    "        rewards.append(reward)\n",
    "    agent.epsilon *= 0.995\n",
    "    \n",
    "    if len(rewards) >= 100:\n",
    "        rewBuf.append(np.mean(rewards[-100:]))\n",
    "    else:\n",
    "        rewBuf.append(np.mean(rewards))\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        clear_output(True)\n",
    "        print(f\"Episode {i*10}\")\n",
    "        print(f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "        print(f\"Recent reward: {rewBuf[-1]:.2f}\")\n",
    "        plt.plot(rewBuf)\n",
    "        plt.xlabel('Episode (x10)')\n",
    "        plt.ylabel('Average Reward (last 100)')\n",
    "        plt.title('Training Progress - CartPole')\n",
    "        plt.show()\n",
    "\n",
    "if rewBuf[-1] > 195:\n",
    "    print(\"Win!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GCEU6g1ddQ5"
   },
   "source": [
    "## 4. Experience replay (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ_3uojDddQ5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        if len(self._storage) < self._maxsize:\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(0, len(self._storage), size=batch_size)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        \n",
    "        for i in indices:\n",
    "            data = self._storage[i]\n",
    "            states.append(data[0])\n",
    "            actions.append(data[1])\n",
    "            rewards.append(data[2])\n",
    "            next_states.append(data[3])\n",
    "            dones.append(data[4])\n",
    "            \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KkoMo-EddQ6"
   },
   "source": [
    "Some tests to make sure your buffer works right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ze99MZrmddQ6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "replay = ReplayBuffer(2)\n",
    "obj1 = tuple(range(5))\n",
    "obj2 = tuple(range(5, 10))\n",
    "replay.add(*obj1)\n",
    "assert replay.sample(1)[0][0] == 0, \"If there's just one object in buffer, it must be retrieved by buf.sample(1)\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage)==2, \"Please make sure __len__ methods works as intended.\"\n",
    "replay.add(*obj2)\n",
    "assert len(replay._storage)==2, \"When buffer is at max capacity, replace objects instead of adding new ones.\"\n",
    "assert np.unique(replay.sample(100)[0])[0] == 5\n",
    "replay.add(*obj1)\n",
    "assert max(len(np.unique(a)) for a in replay.sample(100))==2\n",
    "replay.add(*obj1)\n",
    "assert np.unique(replay.sample(100)[0])[0] == 0\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaImoEEgddQ6"
   },
   "source": [
    "Now let's use this buffer to improve training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czwcYYYmddQ6"
   },
   "outputs": [],
   "source": [
    "env = Binarizer(gym.make('CartPole-v1'))\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    alpha=0.1,\n",
    "    epsilon=0.5,\n",
    "    discount=0.95,\n",
    "    getLegalActions=lambda s: range(n_actions)\n",
    ")\n",
    "replay = ReplayBuffer(10000)\n",
    "\n",
    "def play_and_train_with_replay(env, agent, replay, t_max=500, batch_size=32):\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        action = agent.getAction(s)\n",
    "        next_s, r, done, _ = env.step(action)\n",
    "        \n",
    "        # Заполняем replay buffer\n",
    "        replay.add(s, action, r, next_s, done)\n",
    "        \n",
    "        # Обучаемся на batch из replay buffer\n",
    "        if len(replay) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay.sample(batch_size)\n",
    "            for i in range(batch_size):\n",
    "                agent.update(states[i], actions[i], next_states[i], rewards[i])\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "rewards = []\n",
    "rewBuf = []\n",
    "for i in range(200):\n",
    "    for _ in range(10):\n",
    "        reward = play_and_train_with_replay(env, agent, replay, batch_size=64)\n",
    "        rewards.append(reward)\n",
    "    agent.epsilon *= 0.995\n",
    "    \n",
    "    if len(rewards) >= 100:\n",
    "        rewBuf.append(np.mean(rewards[-100:]))\n",
    "    else:\n",
    "        rewBuf.append(np.mean(rewards))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        clear_output(True)\n",
    "        print(f\"Iteration {i*10}\")\n",
    "        print(f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "        print(f\"Recent reward: {rewBuf[-1]:.2f}\")\n",
    "        plt.plot(rewBuf)\n",
    "        plt.xlabel('Iteration (x10)')\n",
    "        plt.ylabel('Average Reward (last 100)')\n",
    "        plt.title('Training with Experience Replay')\n",
    "        if rewBuf[-1] > 195:\n",
    "            print(\"Win!\")\n",
    "            break\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
